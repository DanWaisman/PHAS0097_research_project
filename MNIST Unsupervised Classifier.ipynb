{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7125e75e",
   "metadata": {},
   "source": [
    "# Unsupervised Classification of MNSIT\n",
    "\n",
    "In this notebook we will be applying an unsupervised classification protocol, for the classification of MNIST images. For the step of feature extraction we will be using both an autoencoder and the VGG-16 pre-trained model. When running this code the user will have to choose one of those to run at a time.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b232a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helper import visualize as vis\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras import models\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Reshape, UpSampling2D, Dropout, Conv2DTranspose, Activation, Concatenate\n",
    "from keras.models import Model, load_model\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style\n",
    "import matplotlib as mpl\n",
    "\n",
    "from scipy.stats import mode, stats\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "import idx2numpy\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008a83e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Importing Datasets & Re-shaping Images.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downloading the MNIST Dataset ###\n",
    "\n",
    "(train_images, train_labels), (test_images_, test_labels_) = mnist.load_data()\n",
    "\n",
    "# Normalisation\n",
    "train_images = train_images / np.max(train_images)\n",
    "test_images_  = test_images_ / np.max(test_images_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9450ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we only classify the last 5000 images from the test set, unseen by our autoencoder up to now.\n",
    "test_images_1 = test_images_[5000:]\n",
    "test_labels   = test_labels_[5000:]\n",
    "\n",
    "# ----------------------- Re shaping ----------------------- #\n",
    "def resize_and_convert( images, shape ):\n",
    "    \n",
    "    images_expanded = tf.expand_dims( images , -1)\n",
    "    \n",
    "    rgb_images = tf.image.grayscale_to_rgb( images_expanded )\n",
    "    \n",
    "    resized_images = tf.image.resize( rgb_images , shape)\n",
    "    \n",
    "    return resized_images\n",
    "# ---------------------------------------------------------- #\n",
    "\n",
    "autoencoder_shape = [64,64]\n",
    "vgg_shape = [224,224]\n",
    "\n",
    "# Reshaping images\n",
    "test_images_auto = resize_and_convert( test_images_1, autoencoder_shape )\n",
    "test_images_vgg = resize_and_convert( test_images_1, vgg_shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a332986",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Loading Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea624a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ====================== Loading Autoencoder Model ====================== ###\n",
    "\n",
    "# Uploading Autoencoder model trained for this purpose\n",
    "model_path = './autoencoder_model_mnist'\n",
    "\n",
    "# loading whole model\n",
    "entire_model = load_model( model_path )\n",
    "\n",
    "### ====================== Loading VGG-16 Model =========================== ###\n",
    "\n",
    "vgg16_path = Path('..','models','VGG16.h5')\n",
    "if not vgg16_path.is_file():\n",
    "    vgg16 = keras.applications.VGG16( include_top=True, weights='imagenet' )\n",
    "    vgg16.save(vgg16_path)\n",
    "    \n",
    "else:   \n",
    "    vgg16 = keras.models.load_model(vgg16_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### =========================== GETTING FEATURE EXTRACTOR =========================== ###\n",
    "\n",
    "def layer_extractor( layer, model ):\n",
    "    \n",
    "    assert layer in [x.name for x in model.layers]  # make sure the layer exists\n",
    "\n",
    "    new_model = keras.Model(inputs = model.input, outputs=[ model.get_layer( layer ).output ])\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "### ===================== Creating Feature Extractor and feature map ================ ###\n",
    "\n",
    "# Getting the feature extractor\n",
    "feature_extractor_auto = layer_extractor('flatten', entire_model)\n",
    "feature_extractor_vgg  = layer_extractor( 'fc1' , vgg16 )\n",
    "\n",
    "# Computing feature map\n",
    "feature_map_auto = feature_extractor_auto.predict( test_images_auto , verbose=True)\n",
    "feature_map_vgg = feature_extractor_vgg.predict( test_images_vgg , verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db55a69",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: PCA Components.\n",
    "\n",
    "- We move forward by choosing one of the two feature maps we have created.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating all PCA components\n",
    "\n",
    "pca_n = PCA(svd_solver='full')\n",
    "\n",
    "x_pca_ = pca_n.fit_transform( feature_map_auto )\n",
    "\n",
    "# Variance per component\n",
    "var_ = pca_n.explained_variance_ratio_.cumsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9fae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOTTING GRAPH: How much variance is kept for a PCA component ###\n",
    "\n",
    "# Plotting\n",
    "fig1 = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# ----------------------------------------------- #\n",
    "ax1 = fig1.add_subplot(1,2,1)\n",
    "ax1.plot( range(1,len(var_)+1), var_ , marker='o')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Number of Components $Log_{10}x$')\n",
    "ax1.set_ylabel('Cumulative Variance')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2 = fig1.add_subplot(1,2,2)\n",
    "ax2.plot( var_ , marker='o')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Cumulative Variance')\n",
    "ax2.grid(True)\n",
    "\n",
    "fig1.suptitle('Cumulative Variance by PCA Components');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aee112",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keeping 50 Components ### Applying whitening in one case and not in the other ###\n",
    "\n",
    "pca_w = PCA(n_components=50, svd_solver='full', whiten=True, random_state=123414)\n",
    "x_w = pca_w.fit_transform( feature_map_auto )\n",
    "\n",
    "pca_nw = PCA(n_components=50, svd_solver='full', whiten=False, random_state=123414)\n",
    "x_nw = pca_nw.fit_transform( feature_map_auto )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d19b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Visualising Clustering feature map through t-SNE.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reducing dimensionality to 2D with tSNE ###\n",
    "\n",
    "tsne_nw = TSNE( n_components=2, random_state=12214 )\n",
    "tsne_w  = TSNE( n_components=2, random_state=654753 )\n",
    "\n",
    "x_nw_tsne = tsne_nw.fit_transform( x_nw )\n",
    "x_w_tsne  = tsne_w.fit_transform( x_w )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a31f9d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plotting on a scatter graph ###\n",
    "\n",
    "fig1 = plt.figure(figsize=(15,5))\n",
    "\n",
    "# --------------------- Plot 1 --------------------- #\n",
    "ax1 = fig1.add_subplot(1,2,1)\n",
    "ax2 = fig1.add_subplot(1,2,2)\n",
    "\n",
    "ax1.scatter( x_nw_tsne[:, 0], x_nw_tsne[:, 1], marker='o' )\n",
    "ax1.set_title('Without Whitening')\n",
    "ax2.scatter( x_w_tsne[:, 0], x_w_tsne[:, 1], marker='o' )\n",
    "ax2.set_title('With Whitening')\n",
    "\n",
    "fig1.suptitle('t-SNE Visualization of Image Features');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5eeea7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: K Means Clustering.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Applying k-means to the Data with whitening applied ###\n",
    "\n",
    "# --------------------------------------------- #\n",
    "\n",
    "kmeans_w = KMeans( n_clusters = 10 , init='k-means++', max_iter=500, n_init=500, random_state=213460)\n",
    "kmeans_w.fit( x_w )\n",
    "\n",
    "labels_unmatched_w = kmeans_w.labels_\n",
    "\n",
    "print('inertia: {:.2f}'.format(kmeans_w.inertia_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af29501",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Applying k-means to the Data with no whitening applied ###\n",
    "\n",
    "kmeans_nw = KMeans( n_clusters = 10 , init='k-means++', max_iter=500, n_init=500, random_state=218460)\n",
    "kmeans_nw.fit( x_nw )\n",
    "\n",
    "labels_unmatched_nw = kmeans_nw.labels_\n",
    "\n",
    "print('inertia: {:.2f}'.format( kmeans_nw.inertia_ ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61f115d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Matching Clusters & Accuracy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e14812",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Checking accuracy Based on MNIST Labels ###\n",
    "\n",
    "def matching_clustering( cluster_labels ):\n",
    "    \n",
    "    # --------------- map the cluster labels to the real Label --------------- #\n",
    "    labels_map = np.zeros_like( cluster_labels )\n",
    "\n",
    "    for i in range(10):\n",
    "        mask = (cluster_labels == i)\n",
    "        labels_map[ mask ] = mode( test_labels[ mask ] )[0]\n",
    "\n",
    "    # --------------- Calculate the accuracy ---------------- #\n",
    "    accuracy = accuracy_score(test_labels, labels_map)\n",
    "\n",
    "    # ----------------- Printing Accuracy ------------------- #\n",
    "    print(f\"\\nTotal Accuracy of K Means clustering: { round(accuracy*100, 2) }%\")\n",
    "\n",
    "    # ========================================================================================== #\n",
    "\n",
    "    print('\\n-----------------------------------------------------------\\n')\n",
    "\n",
    "    cluster_accuracies = np.zeros((10,2))\n",
    "\n",
    "    for i in range(0,10):\n",
    "\n",
    "        positions_cluster_i = np.where( cluster_labels == i )[0]\n",
    "\n",
    "        numbers_in_cluster = test_labels[ positions_cluster_i ]\n",
    "\n",
    "        cluster_real_label = stats.mode( numbers_in_cluster )\n",
    "\n",
    "        cluster_accuracies[i] = [ cluster_real_label[0], round((cluster_real_label[1] / len(numbers_in_cluster)) * 100, 2)]\n",
    "\n",
    "        print(f'Cluster { i }:')\n",
    "        print(f'\\nThe number {cluster_real_label[0]} appears most often...')\n",
    "        print(f'It appears { round((cluster_real_label[1] / len(numbers_in_cluster)) * 100, 2) } % of the time ({cluster_real_label[1]} times).\\n')\n",
    "\n",
    "    # ========================================================================================== #\n",
    "\n",
    "    sorted_indices = cluster_accuracies[:, 0].argsort()\n",
    "    sorted_array = cluster_accuracies[ sorted_indices ]\n",
    "\n",
    "    # ========================================================================================== #\n",
    "    ### Mapping clusters to correct label ###\n",
    "\n",
    "    index = cluster_accuracies[:,0].astype('int')\n",
    "    \n",
    "    # ========================================================================================== #\n",
    "    # Applying the mapping to get the true labels\n",
    "    labels_matched = index[ cluster_labels ]\n",
    "    \n",
    "    plt.figure()\n",
    "\n",
    "    plt.bar( sorted_array[ :, 0 ] , sorted_array[ :, 1 ] )\n",
    "    plt.title('Accuracy per Cluster')\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Accuracy / %')\n",
    "\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return labels_matched, index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc1f33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Computing Accuracies for whitening and no whitening ###\n",
    "\n",
    "labels_w, index_w  = matching_clustering( labels_unmatched_w )\n",
    "\n",
    "labels_nw, index_nw = matching_clustering( labels_unmatched_nw )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f08d277",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### =========================== Comparing to REAL Labels =========================== ###\n",
    "\n",
    "fig1 = plt.figure(figsize=(30,7.5))\n",
    "\n",
    "# --------------------- Plot 1 (k Means Without Whitening) --------------------- #\n",
    "ax1 = fig1.add_subplot(1,3,1)\n",
    "scatter1 = ax1.scatter(x_nw_tsne[:, 0], x_nw_tsne[:, 1], c=labels_nw, cmap='tab10', marker='o')\n",
    "ax1.set_title(f'k Means With No Whitening')\n",
    "cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
    "cbar1.set_label('Label')\n",
    "\n",
    "# --------------------- Plot 2 (k Meas With Whitening) --------------------- #\n",
    "ax2 = fig1.add_subplot(1,3,2)\n",
    "scatter2 = ax2.scatter(x_nw_tsne[:, 0], x_nw_tsne[:, 1], c=labels_w, cmap='tab10', marker='o')\n",
    "ax2.set_title(f'k Means With Whitening')\n",
    "cbar2 = plt.colorbar(scatter2, ax=ax2)\n",
    "cbar2.set_label('Label')\n",
    "\n",
    "# --------------------- Plot 3 (Real Labels) --------------------- #\n",
    "ax3 = fig1.add_subplot(1,3,3)\n",
    "scatter3 = ax3.scatter(x_nw_tsne[:, 0], x_nw_tsne[:, 1], c=test_labels, cmap='tab10', marker='o')\n",
    "ax3.set_title('Real Labels')\n",
    "cbar3 = plt.colorbar(scatter3, ax=ax3, ticks=range(10))\n",
    "cbar3.set_label('Label')\n",
    "\n",
    "fig1.suptitle('Comparing classification results');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675d7ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Further Visualisations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a473f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Looking at the images belonging to each cluster, as classified by k-means clustering ###\n",
    "\n",
    "n_images = 49\n",
    "\n",
    "sides = int(np.sqrt( n_images ))\n",
    "\n",
    "cluster_n = 7\n",
    "positions = np.where( labels_unmatched_w == cluster_n )[0]\n",
    "\n",
    "fig1 = plt.figure( figsize=(25,25) )\n",
    "\n",
    "for i in range( 0, n_images ):\n",
    "\n",
    "    ax = fig1.add_subplot( sides, sides, i+1 )\n",
    "    ax.imshow( test_images_auto[ positions[i] ] )\n",
    "    ax.set_title(f'Number { test_labels[ positions[i] ] }')\n",
    "    \n",
    "fig1.suptitle(f'CLUSTER {cluster_n}, Predicted to be {index_w[cluster_n]}.',fontsize=30);\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa77bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating and designing confusion matrix ###\n",
    "\n",
    "# Creating confusion matrix\n",
    "CM = confusion_matrix( test_labels , labels_nw )\n",
    "\n",
    "# Using helper funciton for visualisation \n",
    "labels_ordered = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "vis.pretty_cm(CM, labels_ordered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00118b49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# using helper function to create visualisation of feature map with images\n",
    "vis.pano_plot(x_nw_tsne[:,0], x_nw_tsne[:,1], test_images_auto, patch_shape=(2, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b89fd4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PCA research\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PCA_components_function( N, k, data, whitening_bool ):\n",
    "    '''\n",
    "    Function which carries out classification protocol for varying values of principal components.\n",
    "    INPUT:\n",
    "    - N              => number of principal components\n",
    "    - k              => k value for k-means clustering\n",
    "    - data           => feature map following feature extraction\n",
    "    - whitening_bool => boolean setting whether whitening is or isnt applied\n",
    "    OUTPUT:\n",
    "    - Percentage accuracy of classification\n",
    "    '''\n",
    "    \n",
    "    # PCA # \n",
    "    pca = PCA( n_components = N, svd_solver='full', whiten=whitening_bool )\n",
    "    pca_data = pca.fit_transform( data )\n",
    "\n",
    "    # K Means clustering #\n",
    "    kmeans_ = KMeans( n_clusters = k, init='k-means++', max_iter=500, n_init=500, random_state= k*50 )\n",
    "    kmeans_.fit( pca_data )\n",
    "    \n",
    "    # Cluster centres and labels #\n",
    "    centers = kmeans_.cluster_centers_\n",
    "    labels = kmeans_.labels_\n",
    "    \n",
    "    # map the cluster labels to the real Label #\n",
    "    labels_map = np.zeros_like( labels )\n",
    "\n",
    "    for i in range(10):\n",
    "        mask = (labels == i)\n",
    "        labels_map[ mask ] = mode( test_labels[ mask ] )[0]\n",
    "\n",
    "    # Calculate the accuracy #\n",
    "    accuracy = accuracy_score(test_labels, labels_map)\n",
    "    \n",
    "    return round(accuracy*100, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df300b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Calculating Accuracies where whitening is applied ###\n",
    "\n",
    "# Principal Component values\n",
    "pca_values = np.array([ 1, 5, 10, 20, 50, 100, 150, 250, 500, 750, 1000, 1800, 3600])\n",
    "accuracies_per_PCA_whitening = np.zeros( len( pca_values ) )\n",
    "\n",
    "whitening = True\n",
    "\n",
    "for i, i_value in tqdm(enumerate(pca_values)):\n",
    "    \n",
    "    accuracies_per_PCA_whitening[i] = PCA_components_function( i_value, 10, feature_map_auto , whitening )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6d5d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Calculating Accuracies where whitening is not applied ###\n",
    "\n",
    "# Principal Component values\n",
    "pca_values = np.array([ 1, 5, 10, 20, 50, 100, 150, 250, 500, 750, 1000, 1800, 3600])\n",
    "accuracies_per_PCA_no_whitening = np.zeros( len( pca_values ) )\n",
    "\n",
    "whitening = False\n",
    "\n",
    "for i, i_value in tqdm(enumerate(pca_values)):\n",
    "    \n",
    "    accuracies_per_PCA_no_whitening[i] = PCA_components_function( i_value, 10, feature_map_auto , whitening )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7daa7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plotting Results ###\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.plot( pca_values, accuracies_per_PCA_no_whitening, label='No Whitening' )\n",
    "plt.plot( pca_values, accuracies_per_PCA_whitening,    label='Whitening' )\n",
    "\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.title('Accuracy of clustering')\n",
    "plt.xlabel('PCA components')\n",
    "plt.ylabel('Accuracy / %')\n",
    "plt.grid()\n",
    "\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd773c",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
