{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7125e75e",
   "metadata": {},
   "source": [
    "# Unsupervised Classification of STM\n",
    "\n",
    "In this notebook we will be applying an unsupervised classification protocol, for the classification of STM images. For the step of feature extraction we will be using both an autoencoder and the VGG-16 pre-trained model. When running this code the user will have to choose one of those to run at a time.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec954d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helper import visualize as vis\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras import models\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Reshape, UpSampling2D, Dropout, Conv2DTranspose, Activation, Concatenate\n",
    "from keras.models import Model, load_model\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style\n",
    "import matplotlib as mpl\n",
    "\n",
    "from scipy.stats import mode, stats\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "import idx2numpy\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008a83e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Importing Dataset and Re-shaping images.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540cfa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing STM Images ###\n",
    "\n",
    "# loading folder\n",
    "path = \"...\"\n",
    "image_folder = [f for f in os.listdir( ... ) if f.endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
    "\n",
    "# ----------- extracting images from folder ----------- #\n",
    "\n",
    "all_images = []\n",
    "\n",
    "for i, image_file in enumerate( image_folder ):\n",
    "    \n",
    "    image_path = os.path.join( path, image_file )\n",
    "    img        = cv2.imread( image_path )\n",
    "    \n",
    "    all_images.append( img )\n",
    "\n",
    "all_images = np.array( all_images )\n",
    "\n",
    "# normalising images\n",
    "all_images = all_images / np.max(all_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9450ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshaping Images ###\n",
    "\n",
    "# ---------------------------------------------------------- #\n",
    "def resize_and_convert( images, shape ):\n",
    "    \n",
    "    resized_images = tf.image.resize( images , shape)\n",
    "    \n",
    "    return resized_images\n",
    "# ---------------------------------------------------------- #\n",
    "\n",
    "autoencoder_shape = [64,64]\n",
    "vgg_shape = [224,224]\n",
    "\n",
    "# Reshaping images\n",
    "test_images_auto = resize_and_convert( all_images, autoencoder_shape )\n",
    "test_images_vgg  = resize_and_convert( all_images, vgg_shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a332986",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Loading Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea624a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ====================== Loading Autoencoder Model ====================== ###\n",
    "\n",
    "# Uploading Autoencoder model trained for this purpose\n",
    "model_path = './autoencoder_model_stm'\n",
    "\n",
    "# loading whole model\n",
    "entire_model = load_model( model_path )\n",
    "\n",
    "### ====================== Loading VGG-16 Model =========================== ###\n",
    "\n",
    "vgg16_path = Path('..','models','VGG16.h5')\n",
    "if not vgg16_path.is_file():\n",
    "    vgg16 = keras.applications.VGG16( include_top=True, weights='imagenet' )\n",
    "    vgg16.save(vgg16_path)\n",
    "    \n",
    "else:   \n",
    "    vgg16 = keras.models.load_model(vgg16_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### =========================== GETTING FEATURE EXTRACTOR =========================== ###\n",
    "\n",
    "def layer_extractor( layer, model ):\n",
    "    \n",
    "    assert layer in [x.name for x in model.layers]  # make sure the layer exists\n",
    "\n",
    "    new_model = keras.Model(inputs = model.input, outputs=[ model.get_layer( layer ).output ])\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "### ===================== Creating Feature Extractor and feature map ================ ###\n",
    "\n",
    "# Getting the feature extractor\n",
    "feature_extractor_auto = layer_extractor('flatten', entire_model)\n",
    "feature_extractor_vgg  = layer_extractor('fc1',     vgg16 )\n",
    "\n",
    "# Computing feature map\n",
    "feature_map_auto = feature_extractor_auto.predict( test_images_auto , verbose=True)\n",
    "feature_map_vgg = feature_extractor_vgg.predict( test_images_vgg , verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db55a69",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: PCA Components.\n",
    "\n",
    "- Whitening will be applied.\n",
    "- $91.89$% variance will be retained.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating all PCA components\n",
    "\n",
    "pca_n = PCA(svd_solver='full', whiten=True)\n",
    "\n",
    "x_pca_ = pca_n.fit_transform( feature_map_vgg )\n",
    "\n",
    "# Cumulative Variance per component\n",
    "var_ = pca_n.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# 92% variance\n",
    "percentage = 0.9189\n",
    "\n",
    "components_to_keep = np.where( var_ >= percentage )[0][0] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9fae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOTTING GRAPH: How much variance is kept for a PCA component ###\n",
    "\n",
    "# Plotting\n",
    "fig1 = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# ----------------------------------------------- #\n",
    "ax1 = fig1.add_subplot(1,2,1)\n",
    "ax1.plot( range(1,len(var_)+1), var_ , marker='o')\n",
    "\n",
    "ax1.hlines( y = percentage , xmin=0, xmax = components_to_keep , colors='red', linestyles='dashed', linewidth=2)\n",
    "ax1.vlines( x = components_to_keep , ymin=0, ymax = percentage , colors='red', linestyles='dashed', linewidth=2)\n",
    "\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "ax1.set_xlabel('Number of Components $Log_{10}x$')\n",
    "ax1.set_ylabel('Cumulative Variance')\n",
    "ax1.grid(True)\n",
    "\n",
    "# ----------------------------------------------- #\n",
    "ax2 = fig1.add_subplot(1,2,2)\n",
    "ax2.plot( var_ , marker='o')\n",
    "\n",
    "ax2.hlines( y = percentage , xmin=0, xmax = components_to_keep , colors='red', linestyles='dashed', linewidth=2)\n",
    "ax2.vlines( x = components_to_keep , ymin=0, ymax = percentage , colors='red', linestyles='dashed', linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Cumulative Variance')\n",
    "ax2.grid(True)\n",
    "\n",
    "fig1.suptitle('Cumulative Variance by PCA Components')\n",
    "\n",
    "print(f'\\n{ percentage *100}% variance retained by {components_to_keep} principal components\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aee112",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keeping 50 Components (which retains 92% of variance as discussed) ###\n",
    "\n",
    "pca_w = PCA( n_components = 50 , svd_solver='full', whiten=True, random_state=123414 )\n",
    "x_w = pca_w.fit_transform( feature_map_vgg )\n",
    "\n",
    "pca_nw = PCA( n_components = 50 , svd_solver='full', whiten=False, random_state=123414 )\n",
    "x_nw = pca_nw.fit_transform( feature_map_vgg )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d19b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Visualising Clustering feature map through t-SNE.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reducing dimensionality to 2D with tSNE ###\n",
    "\n",
    "tsne_w  = TSNE( n_components=2, random_state=654753 )\n",
    "tsne_nw  = TSNE( n_components=2, random_state=654753 )\n",
    "\n",
    "x_w_tsne  = tsne_w.fit_transform( x_w )\n",
    "x_nw_tsne  = tsne_w.fit_transform( x_nw )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a31f9d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plotting on a scatter graph ###\n",
    "\n",
    "fig1 = plt.figure(figsize=(15,5))\n",
    "\n",
    "# --------------------- Plot 1 --------------------- #\n",
    "ax1 = fig1.add_subplot(1,2,1)\n",
    "ax2 = fig1.add_subplot(1,2,2)\n",
    "\n",
    "ax1.scatter( x_nw_tsne[:, 0], x_nw_tsne[:, 1], marker='o' )\n",
    "ax1.set_title('Without Whitening')\n",
    "\n",
    "ax2.scatter( x_w_tsne[:, 0], x_w_tsne[:, 1], marker='o' )\n",
    "ax2.set_title('With Whitening')\n",
    "\n",
    "fig1.suptitle('t-SNE Visualization of Image Features');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff05b755",
   "metadata": {},
   "source": [
    "**Comments**:\n",
    "\n",
    "- In the cases of both the autoencoder and the vgg-16 model, no whitening provided a clearer feature map via t-SNE for visualisation. Therefore the no whitening feature map will be used for visualisation. However for the actual classification only the whitening is being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5eeea7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: K Means Clustering.\n",
    "\n",
    "- Value of **k** is set to 4 (based on observations of the data).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Applying k-means to the Data with whitening applied ###\n",
    "\n",
    "k = 4\n",
    "\n",
    "kmeans_w = KMeans( n_clusters = k , init='k-means++', max_iter=500, n_init=500, random_state=213460)\n",
    "kmeans_w.fit( x_w )\n",
    "\n",
    "labels_unmatched_w = kmeans_w.labels_ \n",
    "\n",
    "print('inertia: {:.2f}'.format(kmeans_w.inertia_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0075f66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### PLOTTING K MEANS CLUSTERING RESULTS ###\n",
    "\n",
    "colors_array = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "                '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "\n",
    "color_labels_w = [colors_array[label] for label in labels_unmatched_w]\n",
    "\n",
    "# plotting\n",
    "\n",
    "fig2 = plt.figure(figsize=(15,10))\n",
    "\n",
    "ax1 = fig2.add_subplot(1,1,1)\n",
    "scatter1 = ax1.scatter( x_nw_tsne[:, 0], x_nw_tsne[:, 1], c=labels_unmatched_w, cmap='tab10', marker='o' )\n",
    "ax1.set_title(f'k Means Without Whitening')\n",
    "cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
    "cbar1.set_label('Cluster Label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675d7ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Further Visualisations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00118b49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "vis.pano_plot(x_nw_tsne[:,0], x_nw_tsne[:,1], all_images, patch_shape=(1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a473f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Images in Cluster ###\n",
    "\n",
    "sides = 6\n",
    "\n",
    "n_images = int( sides**2 )\n",
    "\n",
    "cluster_n = 2\n",
    "positions = np.where( labels_unmatched_w == cluster_n )[0]\n",
    "\n",
    "# -------------------------------- #\n",
    "\n",
    "fig1 = plt.figure( figsize=(25,25) )\n",
    "\n",
    "for i in range( 0, n_images ):\n",
    "\n",
    "    ax = fig1.add_subplot( sides, sides, i+1 )\n",
    "    \n",
    "    ax.imshow( all_images[ positions[i] ] )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f639548a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
